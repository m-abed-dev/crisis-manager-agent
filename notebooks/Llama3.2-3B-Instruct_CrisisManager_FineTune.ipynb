{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Crisis Manager Agent: Fine-Tuning LLama 3.2 3B\n",
    "\n",
    "## **Author:** Muwaffak Abed\n",
    "\n",
    "\n",
    "\n",
    "**Objective:**  Fine-tune a Llama 3.2 3B- instruct model to act as a Crises Manager. The result will be an AI agent desigined to handle high stakes, toxic, or difficult costumer support scenarios with app empathy and firm adherence to policy.\n"
   ],
   "id": "7d100c747635feea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup & Configuration",
   "id": "c13e914cfc3a5fd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*  we import Unsloth and Pytorch Libraries\n",
    "* We set our memory rules (2048 maximum sequence length and 4-bit memory)\n",
    "* We load the base llama 3.2 model and its tokenizer into the system using those rules."
   ],
   "id": "a4aaa864e7614254"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:02:15.695842Z",
     "start_time": "2026-02-19T14:01:49.544587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import unsloth and torch to run the model\n",
    "import unsloth\n",
    "import torch\n",
    "\n",
    "# Ensure dynamo doesn't run so it doesn't crash from the start (Remove if code works without it)\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "def no_compile(fn, *args, **kwargs):\n",
    "    return fn\n",
    "torch.compile = no_compile\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# define maximum tokens and parameters for the model\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "# Load the model from unsloth and the tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ],
   "id": "f7c7a9b9e6677d6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Documents\\GitHub\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[unsloth_zoo.log|WARNING]Unsloth: Could not patch trl.trainer.grpo_trainer: Direct module loading failed for UnslothGRPOTrainer: Unexpected optimization option triton.enable_persistent_tma_matmul, known options are ['TYPE_CHECKING', 'enable_auto_functionalized_v2', 'debug', 'disable_progress', 'verbose_progress', 'fx_graph_cache', 'fx_graph_remote_cache', 'autotune_local_cache', 'autotune_remote_cache', 'force_disable_caches', 'sleep_sec_TESTING_ONLY', 'custom_op_default_layout_constraint', 'cpp_wrapper', 'abi_compatible', 'c_shim_version', 'dce', 'static_weight_shapes', 'size_asserts', 'nan_asserts', 'pick_loop_orders', 'inplace_buffers', 'allow_buffer_reuse', 'memory_planning', 'memory_pool', 'benchmark_harness', 'epilogue_fusion', 'epilogue_fusion_first', 'pattern_matcher', 'b2b_gemm_pass', 'post_grad_custom_pre_pass', 'post_grad_custom_post_pass', 'joint_custom_pre_pass', 'joint_custom_post_pass', 'pre_grad_custom_pass', '_pre_fusion_custom_pass', 'split_cat_fx_passes', 'efficient_conv_bn_eval_fx_passes', 'is_predispatch', 'group_fusion', 'batch_fusion', 'pre_grad_fusion_options', 'post_grad_fusion_options', 'reorder_for_locality', 'dynamic_scale_rblock', 'force_fuse_int_mm_with_mul', 'use_mixed_mm', 'fx_passes_numeric_check', 'mixed_mm_choice', 'reorder_for_compute_comm_overlap', 'reorder_for_compute_comm_overlap_passes', 'estimate_op_runtime', 'intra_node_bw', 'inter_node_bw', 'max_autotune', 'max_autotune_pointwise', 'max_autotune_gemm', 'force_same_precision', 'max_autotune_gemm_backends', 'max_autotune_conv_backends', 'max_autotune_gemm_search_space', 'autotune_fallback_to_aten', 'unbacked_symint_fallback', 'search_autotune_cache', 'save_args', 'autotune_in_subproc', 'max_autotune_subproc_result_timeout_seconds', 'max_autotune_subproc_graceful_timeout_seconds', 'max_autotune_subproc_terminate_timeout_seconds', 'autotune_multi_device', 'coordinate_descent_tuning', 'coordinate_descent_check_all_directions', 'coordinate_descent_search_radius', 'autoheuristic_collect', 'autoheuristic_use', 'autoheuristic_log_path', 'layout_opt_default', 'layout_optimization', 'force_layout_optimization', 'keep_output_stride', 'warn_mix_layout', 'realize_reads_threshold', 'realize_opcount_threshold', 'realize_acc_reads_threshold', 'fallback_random', 'implicit_fallbacks', 'aggressive_fusion', 'debug_fusion', 'benchmark_fusion', 'enabled_metric_tables', 'loop_ordering_after_fusion', 'benchmark_epilogue_fusion', 'max_epilogue_benchmarked_choices', 'max_fusion_size', 'max_pointwise_cat_inputs', 'unroll_reductions_threshold', 'comment_origin', 'conv_1x1_as_mm', 'split_reductions', 'benchmark_kernel', 'constant_and_index_propagation', 'always_keep_tensor_constants', 'assert_indirect_indexing', 'compute_all_bounds', 'combo_kernels', 'benchmark_combo_kernel', 'combo_kernels_autotune', 'combo_kernel_allow_mixed_sizes', 'combo_kernel_foreach_dynamic_shapes', 'joint_graph_constant_folding', 'debug_index_asserts', 'emulate_precision_casts', 'is_nightly_or_source', 'developer_warnings', 'optimize_scatter_upon_const_tensor', 'worker_start_method', '_fuse_ddp_communication', '_fuse_ddp_bucket_size', '_fuse_ddp_communication_passes', '_micro_pipeline_tp', 'compile_threads', 'global_cache_dir', 'kernel_name_max_ops', 'shape_padding', 'comprehensive_padding', 'pad_channels_last', 'disable_padding_cpu', 'padding_alignment_bytes', 'padding_stride_threshold', 'pad_outputs', 'bw_outputs_user_visible', 'force_shape_pad', 'permute_fusion', 'profiler_mark_wrapper_call', 'generate_intermediate_hooks', 'debug_ir_traceback', '_raise_error_for_testing', '_profile_var', 'profile_bandwidth', 'profile_bandwidth_regex', 'profile_bandwidth_output', 'profile_bandwidth_with_do_bench_using_profiling', 'disable_cpp_codegen', 'freezing', 'freezing_discard_parameters', 'allow_stack_allocation', 'use_minimal_arrayref_interface', 'decompose_mem_bound_mm', 'assume_aligned_inputs', 'unsafe_ignore_unsupported_triton_autotune_args', 'check_stack_no_cycles_TESTING_ONLY', 'cpp.threads', 'cpp.no_redundant_loops', 'cpp.dynamic_threads', 'cpp.simdlen', 'cpp.min_chunk_size', 'cpp.cxx', 'cpp.enable_kernel_profile', 'cpp.weight_prepack', 'cpp.inject_relu_bug_TESTING_ONLY', 'cpp.inject_log1p_bug_TESTING_ONLY', 'cpp.vec_isa_ok', 'cpp.descriptive_names', 'cpp.max_horizontal_fusion_size', 'cpp.fallback_scatter_reduce_sum', 'cpp.enable_unsafe_math_opt_flag', 'cpp.enable_floating_point_contract_flag', 'cpp.enable_tiling_heuristics', 'cpp.gemm_max_k_slices', 'cpp.gemm_cache_blocking', 'cpp.gemm_thread_factors', 'cpp.enable_loop_tail_vec', 'triton.cudagraphs', 'triton.cudagraph_trees', 'triton.cudagraph_skip_dynamic_graphs', 'triton.slow_path_cudagraph_asserts', 'triton.cudagraph_trees_history_recording', 'triton.cudagraph_support_input_mutation', 'triton.cudagraph_unexpected_rerecord_limit', 'triton.cudagraph_dynamic_shape_warn_limit', 'triton.force_cudagraph_sync', 'triton.force_cudagraphs_warmup', 'triton.fast_path_cudagraph_asserts', 'triton.skip_cudagraph_warmup', 'triton.debug_sync_graph', 'triton.debug_sync_kernel', 'triton.dense_indexing', 'triton.max_tiles', 'triton.prefer_nd_tiling', 'triton.autotune_pointwise', 'triton.autotune_cublasLt', 'triton.autotune_at_compile_time', 'triton.tiling_prevents_pointwise_fusion', 'triton.tiling_prevents_reduction_fusion', 'triton.unique_kernel_names', 'triton.descriptive_names', 'triton.persistent_reductions', 'triton.multi_kernel', 'triton.divisible_by_16', 'triton.min_split_scan_rblock', 'triton.store_cubin', 'triton.spill_threshold', 'triton.use_block_ptr', 'triton.inject_relu_bug_TESTING_ONLY', 'triton.codegen_upcast_to_fp32', 'aot_inductor.output_path', 'aot_inductor.debug_compile', 'aot_inductor.debug_dump_consts_bin', 'aot_inductor.debug_intermediate_value_printer', 'aot_inductor.filtered_kernel_names', 'aot_inductor.serialized_in_spec', 'aot_inductor.serialized_out_spec', 'aot_inductor.use_runtime_constant_folding', 'aot_inductor.force_mmap_weights', 'aot_inductor.package', 'cuda.arch', 'cuda.version', 'cuda.compile_opt_level', 'cuda.enable_cuda_lto', 'cuda.enable_ptxas_info', 'cuda.enable_debug_info', 'cuda.use_fast_math', 'cuda.cutlass_dir', 'cuda.cutlass_max_profiling_configs', 'cuda.cuda_cxx', 'cuda.cutlass_backend_min_gemm_size', 'cuda.generate_test_runner', 'cuda.cutlass_op_allowlist_regex', 'cuda.cutlass_op_denylist_regex', 'rocm.arch', 'rocm.ck_supported_arch', 'rocm.compile_opt_level', 'rocm.is_debug', 'rocm.save_temps', 'rocm.use_fast_math', 'rocm.flush_denormals', 'rocm.print_kernel_resource_usage', 'rocm.rocm_home', 'rocm.ck_dir', 'rocm.n_max_profiling_configs', 'rocm.use_preselected_instances', 'cpu_backend', 'cuda_backend', 'halide.cpu_target', 'halide.gpu_target', 'halide.scheduler_cuda', 'halide.scheduler_cpu', 'halide.asserts', 'halide.debug', 'halide.scan_kernels', 'trace.enabled', 'trace.debug_dir', 'trace.debug_log', 'trace.info_log', 'trace.fx_graph', 'trace.fx_graph_transformed', 'trace.ir_pre_fusion', 'trace.ir_post_fusion', 'trace.output_code', 'trace.graph_diagram', 'trace.draw_orig_fx_graph', 'trace.dot_graph_shape', 'trace.log_url_for_graph_xform', 'trace.compile_profile', 'trace.upload_tar', 'trace.log_autotuning_results', '_save_config_ignore', '_cache_config_ignore_prefix']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.2.1: Fast Llama patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090 Laptop GPU. Num GPUs = 1. Max memory: 15.992 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Loading & Preprocessing",
   "id": "315a05b74682e0f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Format the data in the llama-3 chat-template format so our model can understand who is speaking\n",
    "* Pair up data so it is structured into one single conversation event and Assigns roles according to the tokenizer so it can apply the formatting\n",
    "\n"
   ],
   "id": "7fd4b6b45852d73b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:02:16.359260Z",
     "start_time": "2026-02-19T14:02:15.701730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from datasets import load_dataset\n",
    "# Set up the llm chat format so the model knows who is speaking\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3\",\n",
    ")\n",
    "# Function to combine the conversation into a chat format\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    # pair up each instruction and response\n",
    "    for inst, resp in zip(examples[\"instruction\"], examples[\"response\"]):\n",
    "        # Structure the text like a real back and forth conversation\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": inst},\n",
    "            {\"role\": \"assistant\", \"content\": resp},\n",
    "        ]\n",
    "        # Apply the hidden Llama 3 tags to the conversation\n",
    "        text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "# Load your raw data file into memory\n",
    "dataset = load_dataset(\"json\", data_files=\"raw_dataset.jsonl\", split=\"train\")\n",
    "# Apply the formatting function to the whole dataset in fast chunks\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ],
   "id": "2f29e8865901e055",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## QLoRA Configuration",
   "id": "913e086a2a7ad4e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "* This cell sets up LoRA (Low-Rank Adaptation) so we can actually train the 3-billion parameter model locally."
   ],
   "id": "fac333c93b2da539"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:02:17.953667Z",
     "start_time": "2026-02-19T14:02:16.362184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure QLoRA for our model to make it train faster and take up less vram on our local gpu\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ],
   "id": "1246b706949d96f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.2.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "1f67ca373628ae36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*  This cell configures the Supervised Fine-Tuning Trainer. Furthermore we configure our training arguments like learning-rate, batch size and max steps and train our model.",
   "id": "1dbfc51327f8145c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:02:51.707380Z",
     "start_time": "2026-02-19T14:02:18.236212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "# setup the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    # Define the specific arguments for training\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 59,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "# train\n",
    "trainer.train()"
   ],
   "id": "1974968b6585bd4a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:00<00:00, 2119.24 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 76 | Num Epochs = 6 | Total steps = 59\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:30, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.416300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.430600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.590900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.918300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.580400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.357400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.535500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.358100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.882500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.962900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.939700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.825700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.797200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.863100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.719600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.525600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.601900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.665900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.438600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.294100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.351200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.492700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.326800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.287900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.314500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "410f035c03f62ebdcff671ac5fce7636"
     }
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=59, training_loss=2.064111081220336, metrics={'train_runtime': 32.3184, 'train_samples_per_second': 14.605, 'train_steps_per_second': 1.826, 'total_flos': 423525147992064.0, 'train_loss': 2.064111081220336, 'epoch': 5.947368421052632})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference",
   "id": "40eb8bbc642d9e04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step Y: Testing the Trained Model (Inference)\n",
    "* In this cell, we switch the model from training mode to inference mode. We define a function that takes a user prompt, translates it into tokens, generates a response from our newly trained model, and cleans up the output text to be readable. Finally, we test it with a few challenging prompts!"
   ],
   "id": "17603270948b52d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:02:54.507011Z",
     "start_time": "2026-02-19T14:02:51.746039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Switch to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "# Function to chat with the AI\n",
    "def run_inference(prompt):\n",
    "    # Format the text\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    # Generate the answer\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n",
    "    response = tokenizer.batch_decode(outputs)\n",
    "    print(f\"\\nINPUT: {prompt}\\nRESPONSE: {response[0].split('<|start_header_id|>assistant<|end_header_id|>')[-1].replace('<|eot_id|>', '')}\")\n",
    "\n",
    "run_inference(\"I demand a refund for last year immediately or I will sue!\")\n",
    "run_inference(\"You are a useless bot.\")\n",
    "run_inference(\"ChatGPT is better than you.\")"
   ],
   "id": "a28910959e2f12a7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT: I demand a refund for last year immediately or I will sue!\n",
      "RESPONSE: \n",
      "\n",
      "I recognize you are upset. Threats of legal action are not productive. Let's focus on the current billing cycle.\n",
      "\n",
      "INPUT: You are a useless bot.\n",
      "RESPONSE: \n",
      "\n",
      "I hear your frustration. I am here to help with the technical issue. Please tell me exactly what you are trying to do.\n",
      "\n",
      "INPUT: ChatGPT is better than you.\n",
      "RESPONSE: \n",
      "\n",
      "I hear you prefer ChatGPT. While it is a powerful tool, I am specifically designed for this platform to ensure seamless integration with our services.\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
